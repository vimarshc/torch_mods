Implementing research papers



The ptb-lm has been inspired from an assignment from Montreal Institute of Learning Algorithms. Added to serve as a basic API which can be manipulated easily. 

The data has also been taken from the same source, however it definitely looks like the 
data has been taken from Mikolov, thus the *partitioning* is the same is as the following: 
	* http://www.fit.vutbr.cz/~imikolov/rnnlm/char.pdf (sub word Language model)
	* https://arxiv.org/abs/1603.09025 (recurrent batch norm)


Implemented (With TODO):
	* https://arxiv.org/abs/1511.08400
		Implemented the norm stabilizer which can be found in rnn_optim.
		* Would like to test out with PTB at a token level.

		I think this version is production ready.

	* Basic Version of a Hierarchical RNN
		This has been implemented using single layer LSTM (Can be replaced with LSTM cells)
		The current version takes no boundry decisions.

		This ideas was inspired from https://arxiv.org/pdf/1609.01704.pdf. 
		Where the paper implements a mechanism to detect boundry while processing the input. 
		Right now the version implemented in this paper does no such activity.

			todo: 
				* Implement the boundry decision   

	* Added basic structure for a ClockWork RNN.
	* Added basic forward prop of vanilla (Pytorch RNN)  

TODO 
	Infrastructure:
		* Add Graphs from MILA assignment.
		* Add tests for clockwork RNN. 
		* Add graphs from clockwork RNN paper. 
		* Add HRNN, CWRNN in config.  

	(Papers/Ideas):
	* Recurrent Batch Norm: https://arxiv.org/abs/1603.09025
		Originally I thought this could be easily achieved with an LSTM cell, but upon further inspection I don't think that's possible. 
		While have to implement the LSTM in vanilla form and inject BatchNorm

	* Improvements from https://arxiv.org/pdf/1212.0901.pdf

	* Stack RNN
		Paper is here[1] and C++ implementation is here[2]. Paper is by Mikolov where they're using a stack actions to retain memory. Has a discussion where we could use multiple stacks in parallel and have have operations to manipulate memory amongst the parallel stacks.   

		[1]https://papers.nips.cc/paper/5857-inferring-algorithmic-patterns-with-stack-augmented-recurrent-nets.pdf
		[2] https://github.com/facebook/Stack-RNN
	
	* Ideas from: 
		* https://arxiv.org/pdf/1211.5063.pdf
		* https://www.cs.toronto.edu/~fritz/absps/momentum.pdf

		Both papers highlight the importance of good initilizations. 
			* [2] used normal distribution for it's experiments, however emphasizes that the scale of the initilizations vary upon the task at hand. 
			* Both papers emphasize the importance of the spectal radius of the initilizations. 

			Implement function to draw from sample and ensure a spectral radius of 1. 

	* Clockwork RNN: 
		https://arxiv.org/abs/1402.3511
		 Added code for forward prop. Need to add tests, debug and verify Backward prop works as mentioned in paper and can be handled as is by PyTorch. There's a discussion in the paper about how the time period allocation can be trained as well which willl require the modulo function to be differentiable. 
	
	* MultiScale HRNN:
		https://arxiv.org/pdf/1609.01704.pdf
		Includes a non-differentiable function which is handled in some way. 


	* Word Embeddings
		* https://arxiv.org/pdf/1310.4546.pdf
		The overall implementation is relatively straightword but PyTorch lacks support for NCE loss. 
		

		Is available in tensorflow[1]. TensorFlow samples negative examples using a log uniform candidate sampler which can be found here[3] which expects input sorted in decresing order of frequency. 

		PyTorch does not have support for a log uniform sampler but has been discussed here.[2]
		Thinking of having a peak at how negative sampling has been achieved in word2vec. Adding the source code here for reference. 

		[4] Introduces Few Advances in Pre-Trainning Word Embeddings. 
		

		
		[1] https://github.com/tensorflow/tensorflow/blob/r1.13/tensorflow/python/ops/nn_impl.py#L1466
		[2] https://github.com/pytorch/pytorch/issues/3718
		[3]https://github.com/tensorflow/tensorflow/blob/r1.13/tensorflow/python/ops/nn_impl.py#L1360
		[4] https://arxiv.org/pdf/1712.09405.pdf

Interesteing in Future Work: 
	* Hessian Free Optimization:
		http://www.cs.toronto.edu/~jmartens/docs/Deep_HessianFree.pdf
		A small search yeilds the idea that this can't be implemented in PyTorch using the autograd framework. Though Martens has released his MATLAB implementation: 
		http://www.cs.toronto.edu/~jmartens/docs/HFDemo.zip

	* Fast Linear Model for Knowledge Graph Embeddings 
	https://arxiv.org/pdf/1710.10881.pdf

	* Differential State Framework
	https://arxiv.org/pdf/1703.08864.pdf

Contributing:
	* NormStabilizer:
		* Works for Sequence? 
		* Works for multiple layers?

